{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 블로그 원본 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 정보 가져오기 및 headers 세팅\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "            'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "# 뉴스 제목 url\n",
    "url = \"https://www.investing.com/crypto/bitcoin/news\"\n",
    "# http 요청 받기\n",
    "response = requests.get(url,headers=headers)\n",
    "# url에 대한 정보를 받을 수 있는 soup 생성\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "news_title=[title.get('title') for title in soup.select('.title') if title.get('title') != None]\n",
    "print(news_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### gpt 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# investing.com에서 BTC 뉴스 페이지 URL\n",
    "url = 'https://www.investing.com/crypto/bitcoin/news'\n",
    "\n",
    "# HTTP GET 요청을 보내서 페이지 내용 가져오기\n",
    "response = requests.get(url)\n",
    "\n",
    "# 요청이 성공했는지 확인\n",
    "if response.status_code == 200:\n",
    "    # 페이지 내용을 BeautifulSoup을 사용하여 파싱\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 뉴스 제목들을 찾기 위한 CSS 클래스 확인\n",
    "    news_titles = soup.select('.largeTitle a')\n",
    "\n",
    "    # 각 뉴스 제목 출력\n",
    "    for title in news_titles:\n",
    "        print(title.text.strip())\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')\n",
    "    \n",
    "print(news_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### gpt 수정 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# html 정보 가져오기 및 headers 세팅\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'\n",
    "}\n",
    "\n",
    "# 뉴스 제목 url\n",
    "url = \"https://www.investing.com/crypto/bitcoin/news\"\n",
    "\n",
    "# http 요청 받기\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# url에 대한 정보를 받을 수 있는 soup 생성\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 뉴스 제목 선택\n",
    "news_titles = [title.text.strip() for title in soup.select('.font-bold.hover\\\\:underline')]\n",
    "\n",
    "# 결과 출력\n",
    "print(news_titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 자동 크롬드라이버 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Chrome 브라우저를 백그라운드에서 실행하도록 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # 백그라운드 실행 옵션 추가\n",
    "\n",
    "# 특정 버전의 ChromeDriver 사용\n",
    "chrome_driver_path = ChromeDriverManager(browser_version=\"114.0.5735.90\").install()\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Selenium으로 웹페이지 열기\n",
    "url = \"https://www.investing.com/crypto/bitcoin/news\"\n",
    "driver.get(url)\n",
    "time.sleep(5)  # 페이지가 로딩되기를 기다림 (시간이 많이 걸릴 경우 조절 필요)\n",
    "\n",
    "# 현재 페이지 소스코드 가져오기\n",
    "page_source = driver.page_source\n",
    "\n",
    "# BeautifulSoup을 사용하여 뉴스 제목 추출\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "news_titles = [title.text.strip() for title in soup.select('.font-bold.hover\\\\:underline')]\n",
    "\n",
    "# 결과 출력\n",
    "print(news_titles)\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 수동 경로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Enforcement Directorate arrests Katalyst executive in Bitcoin scam', 'Shiba Inu (SHIB) Becoming Bullish, Ethereum (ETH) Price Screams Rally Continuation, Bitcoin (BTC) Not Giving up Market Dominance', 'Crypto social media accounts hacked, spreading misinformation', 'BTIG upgrades Marathon Digital as Bitcoin draws attention after spot ETF approvals', 'Binance Issues Important Notice for BTC, XRP, ADA Futures Traders: Details', 'VanEck to liquidate Bitcoin Strategy ETF following SEC nod', \"BlackRock to Become BTC Biggest Holder, Analyst Predicts; Michael Saylor Issues Warning for BTC Holders; SHIB Rep Unveils Shibarium's Future: Crypto News Digest by U.Today\", 'JPMorgan CEO remains skeptical about Bitcoin as price dips', \"Max Keiser Points to BTC Price Growth Estimate, Justin Sun Withdraws $13.8 Million ETH From Binance, Elon Musk's Post Sparks SHIB, XRP Armies' Curiosity: Crypto News Digest by U.Today\", 'Ethereum (ETH) Makes Comeback, Finally', 'Find a Crypto Broker']\n",
      "뉴스 헤더가 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Chrome 브라우저를 백그라운드에서 실행하도록 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # 백그라운드 실행 옵션 추가\n",
    "\n",
    "# 크롬 드라이버 다운로드 및 실행 경로 지정\n",
    "chrome_driver_path = 'C:\\education\\ml_dev\\coin\\jh_-\\investing\\chromedriver-win64'  # 자신의 크롬 드라이버 경로로 변경\n",
    "\n",
    "# Selenium으로 웹페이지 열기\n",
    "url = \"https://www.investing.com/crypto/bitcoin/news\"\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(url)\n",
    "time.sleep(1)  # 페이지가 로딩되기를 기다림 (시간이 많이 걸릴 경우 조절 필요)\n",
    "\n",
    "# 현재 페이지 소스코드 가져오기\n",
    "page_source = driver.page_source\n",
    "\n",
    "# BeautifulSoup을 사용하여 뉴스 제목 추출\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "news_titles = [title.text.strip() for title in soup.select('.font-bold.hover\\\\:underline')]\n",
    "\n",
    "# 결과 출력\n",
    "print(news_titles)\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 뉴스 헤더를 JSON 파일로 저장\n",
    "result_data = {'news_titles': news_titles}\n",
    "with open('bitcoin_news_titles.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(result_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"뉴스 헤더가 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 전체 페이지에 대한 뉴스 제목과 본문을 json로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 헤더가 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Chrome 브라우저를 백그라운드에서 실행하도록 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # 백그라운드 실행 옵션 추가\n",
    "\n",
    "# 크롬 드라이버 다운로드 및 실행 경로 지정 (자신의 크롬 드라이버 경로로 변경)\n",
    "chrome_driver_path = 'C:\\\\education\\\\ml_dev\\\\coin\\\\jh_-\\\\investing\\\\chromedriver-win64'\n",
    "\n",
    "# Selenium으로 웹페이지 열기 함수\n",
    "def get_news_titles(url):\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # 페이지가 로딩되기를 기다림 (시간이 많이 걸릴 경우 조절 필요)\n",
    "    \n",
    "    # 현재 페이지 소스코드 가져오기\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # BeautifulSoup을 사용하여 뉴스 제목 추출\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    news_titles = [title.text.strip() for title in soup.select('.textDiv')]\n",
    "    \n",
    "    # 드라이버 종료\n",
    "    driver.quit()\n",
    "    \n",
    "    return news_titles\n",
    "\n",
    "# 여러 페이지의 뉴스 헤더를 저장할 리스트\n",
    "all_news_titles = []\n",
    "\n",
    "# 1부터 100까지의 페이지에 대해 뉴스 헤더를 가져와서 리스트에 추가\n",
    "for page_number in range(1, 2):\n",
    "    url = f\"https://www.investing.com/news/cryptocurrency-news/{page_number}\"\n",
    "    news_titles = get_news_titles(url)\n",
    "    all_news_titles.extend(news_titles)\n",
    "\n",
    "# 뉴스 헤더를 JSON 파일로 저장\n",
    "result_data = {'news_titles': all_news_titles}\n",
    "with open('2all_bitcoin_news_titles.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(result_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"뉴스 헤더가 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- (과정)뉴스 헤더와 시간 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에러가 발생했습니다. 페이지: https://www.investing.com/news/cryptocurrency-news/1\n",
      "에러가 발생하여 프로그램을 종료합니다. 페이지: 1\n",
      "\n",
      "총 0개의 뉴스 정보 크롤링 완료 \n",
      "뉴스 정보가 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Chrome 브라우저를 백그라운드에서 실행하도록 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # 백그라운드 실행 옵션 추가\n",
    "\n",
    "# 크롬 드라이버 다운로드 및 실행 경로 지정 (자신의 크롬 드라이버 경로로 변경) 한번 실행하고 끝\n",
    "# chrome_driver_path = 'C:\\\\education\\\\ml_dev\\\\coin\\\\jh_-\\\\investing\\\\chromedriver-win64'\n",
    "\n",
    "# Selenium으로 웹페이지 열기 함수\n",
    "def get_news_info(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # 페이지가 로딩되기를 기다림 (시간이 많이 걸릴 경우 조절 필요)\n",
    "\n",
    "        # 현재 페이지 소스코드 가져오기\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # BeautifulSoup을 사용하여 뉴스 제목과 올라온 시간 추출\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')    \n",
    "\n",
    "        news_info_list = []\n",
    "\n",
    "        # 뉴스 제목과 올라온 시간이 포함된 요소 선택\n",
    "        news_elements = soup.select('.largeTitle') \n",
    "\n",
    "        for element in news_elements:\n",
    "            # largeTitle 클래스 내에서 모든 .title과 .date를 찾음\n",
    "            titles = element.select('.title')\n",
    "            dates = element.select('.date')\n",
    "\n",
    "            # 뉴스 제목과 올라온 시간이 있는 경우에만 결과 리스트에 추가\n",
    "            for title, date in zip(titles, dates):\n",
    "                title_text = title.text.strip()\n",
    "                date_text = date.text.strip()\n",
    "                news_info_list.append({'title': title_text, 'time': date_text})\n",
    "                start_nm += 1 # 다음 번호 증가시킴\n",
    "\n",
    "        # 드라이버 종료\n",
    "        driver.quit()\n",
    "\n",
    "        return news_info_list\n",
    "\n",
    "    except Exception as e:\n",
    "        # 에러가 발생했을 때 현재 페이지 번호를 출력하고 예외를 다시 발생시킴\n",
    "        print(f\"에러가 발생했습니다. 페이지: {url}\")\n",
    "        raise e\n",
    "\n",
    "# 여러 페이지의 뉴스 정보를 저장할 리스트\n",
    "all_news_info = []\n",
    "\n",
    "# 1부터 2까지의 페이지에 대해 뉴스 정보를 가져와서 리스트에 추가\n",
    "for page_number in range(1, 3):\n",
    "    url = f\"https://www.investing.com/news/cryptocurrency-news/{page_number}\"\n",
    "    try:\n",
    "        news_info = get_news_info(url)\n",
    "        all_news_info.extend(news_info)\n",
    "    except Exception as e:\n",
    "        # 에러가 발생했을 때 현재 페이지 번호를 출력하고 반복문을 종료\n",
    "        print(f\"에러가 발생하여 프로그램을 종료합니다. 페이지: {page_number}\")\n",
    "        break\n",
    "\n",
    "# 뉴스 정보를 출력하는 코드\n",
    "# for i, news in enumerate(all_news_info, 1):\n",
    "    # print(f\"{i}. 뉴스 헤더: {news['title']}, 업로드 시간:{news['time']}\")\n",
    "    \n",
    "# 총 몇개의 데이터를 가져왔는지 출력\n",
    "print(f\"\\n총 {len(all_news_info)}개의 뉴스 정보 크롤링 완료 \")\n",
    "\n",
    "# 뉴스 정보를 JSON 파일로 저장\n",
    "result_data = {'news_info': all_news_info}\n",
    "with open('all_cryptocurrency_news_info.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(result_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"뉴스 정보가 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- (실행)뉴스 헤더와 업로드 시간"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "총 171개의 뉴스 정보 크롤링 완료 \n",
      "뉴스 정보가 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
>>>>>>> 569e9a916dc7df956c3ca7b3fe5719758e0dba3c
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def get_news_info(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # 오류가 발생하면 예외를 발생시킴\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        news_info_list = []\n",
    "\n",
    "        # 뉴스 제목과 올라온 시간이 포함된 요소 선택\n",
    "        news_elements = soup.select('.largeTitle')\n",
    "\n",
    "        for element in news_elements:\n",
    "            # largeTitle 클래스 내에서 모든 .title과 .date를 찾음\n",
    "            titles = element.select('.title')\n",
    "            dates = element.select('.date')\n",
    "\n",
    "            # 뉴스 제목과 올라온 시간이 있는 경우에만 결과 리스트에 추가\n",
    "            for title, date in zip(titles, dates):\n",
    "                title_text = title.text.strip()\n",
    "                date_text = date.text.strip()\n",
    "                news_info_list.append({'title': title_text, 'time': date_text})\n",
    "\n",
    "        return news_info_list\n",
    "\n",
    "    except Exception as e:\n",
    "        # 에러가 발생했을 때 현재 페이지 번호를 출력하고 예외를 다시 발생시킴\n",
    "        print(f\"에러가 발생했습니다. 페이지: {url}\")\n",
    "        raise e\n",
    "\n",
    "# 여러 페이지의 뉴스 정보를 저장할 리스트\n",
    "all_news_info = []\n",
    "\n",
    "# 1부터 2까지의 페이지에 대해 뉴스 정보를 가져와서 리스트에 추가\n",
<<<<<<< HEAD
    "for page_number in range(3, 3792):\n",
=======
    "for page_number in range(1, 10):\n",
>>>>>>> 569e9a916dc7df956c3ca7b3fe5719758e0dba3c
    "    url = f\"https://www.investing.com/news/cryptocurrency-news/{page_number}\"\n",
    "    try:\n",
    "        news_info = get_news_info(url)\n",
    "        all_news_info.extend(news_info)\n",
    "        result_data = {'news_info': all_news_info}\n",
    "        with open('all_cryptocurrency_news_info.json', 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(result_data, json_file, ensure_ascii=False, indent=4)\n",
    "        print(page_number)\n",
    "    except Exception as e:\n",
    "        # 에러가 발생했을 때 현재 페이지 번호를 출력하고 반복문을 종료\n",
    "        print(f\"에러가 발생하여 프로그램을 종료합니다. 페이지: {page_number}\")\n",
    "        break\n",
    "\n",
    "# 총 몇개의 데이터를 가져왔는지 출력\n",
    "print(f\"\\n총 {len(all_news_info)}개의 뉴스 정보 크롤링 완료 \")\n",
    "\n",
    "# 뉴스 정보를 JSON 파일로 저장\n",
    "# result_data = {'news_info': all_news_info}\n",
    "# with open('all_cryptocurrency_news_info.json', 'w', encoding='utf-8') as json_file:\n",
    "#     json.dump(result_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"뉴스 정보가 성공적으로 저장되었습니다.\")\n",
    "asd=12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- (본문)뉴스 본문 접속 후 헤더와 업로드시간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에러가 발생하여 프로그램을 종료합니다. 페이지: 1\n",
      "\n",
      "총 0개의 뉴스 정보 크롤링 완료 \n",
      "뉴스 정보가 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# Chrome 브라우저를 백그라운드에서 실행하도록 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # 백그라운드 실행 옵션 추가\n",
    "\n",
    "# Selenium으로 웹페이지 열기 함수\n",
    "def get_news_info(url):\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # 페이지가 로딩되기를 기다림 (시간이 많이 걸릴 경우 조절 필요)\n",
    "\n",
    "    # 현재 페이지 소스코드 가져오기\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # BeautifulSoup을 사용하여 뉴스 본문 페이지 파싱\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')    \n",
    "\n",
    "    news_info_list = []\n",
    "\n",
    "    # 뉴스 본문에 있는 헤더와 업로드 시간 가져오기\n",
    "    header = soup.find()('h1', class_='articleHeader')\n",
    "    upload_time = soup.find('div', class_='contentSectionDetails')\n",
    "\n",
    "    # 가져온 데이터를 딕셔너리에 저장하여 결과 리스트에 추가\n",
    "    if header and upload_time:\n",
    "        title_text = header.text.strip()\n",
    "        time_text = upload_time.text.strip()\n",
    "        news_info_list.append({'title': title_text, 'time': time_text})\n",
    "\n",
    "    # 드라이버 종료\n",
    "    driver.quit()\n",
    "\n",
    "    return news_info_list\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     # 에러가 발생했을 때 현재 페이지 번호를 출력하고 예외를 다시 발생시킴\n",
    "    #     print(f\"에러가 발생했습니다. 페이지: {url}\")\n",
    "    #     raise e\n",
    "\n",
    "# 여러 페이지의 뉴스 정보를 저장할 리스트\n",
    "all_news_info = []\n",
    "\n",
    "# 1부터 2까지의 페이지에 대해 뉴스 정보를 가져와서 리스트에 추가\n",
    "for page_number in range(1, 3):\n",
    "    url = f\"https://www.investing.com/news/cryptocurrency-news/{page_number}\"\n",
    "    try:\n",
    "        news_info = get_news_info(url)\n",
    "        all_news_info.extend(news_info)\n",
    "    except Exception as e:\n",
    "        # 에러가 발생했을 때 현재 페이지 번호를 출력하고 반복문을 종료\n",
    "        print(f\"에러가 발생하여 프로그램을 종료합니다. 페이지: {page_number}\")\n",
    "        break\n",
    "    \n",
    "# 총 몇개의 데이터를 가져왔는지 출력\n",
    "print(f\"\\n총 {len(all_news_info)}개의 뉴스 정보 크롤링 완료 \")\n",
    "\n",
    "# 뉴스 정보를 JSON 파일로 저장\n",
    "result_data = {'news_info': all_news_info}\n",
    "with open('all_cryptocurrency_news_info.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(result_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"뉴스 정보가 성공적으로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- Long Polling\n",
    "    - 서버 커넥션이 끊기자마자 바로 재요청해서 서버 커넥션 오픈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주식 관련 데이터는 vuex store에서 관리\n",
    "async subscribeStocks() {\n",
    "    await this.getStocks(); # 주식을 가져오는 action dispatch\n",
    "    \n",
    "    # getStocks action 내의 trycatch 문으로 에러를 잡아 stocksIsError 토클링\n",
    "    if (this.stocksIsError) {\n",
    "        this.setStocksIsError(false); # 일단 에러 무시\n",
    "        await new Promise((resolve) => setTimeout(resolve, 1000)); # 1초만 대기\n",
    "    }\n",
    "    \n",
    "    this.subscribeStocks() # 커넥션 끊기자마자 재귀호출하여 커넥션 유지\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 실시간으로 해당 뉴스 페이지에서 뉴스가 업로드되면 헤더와 업로드시간을 찍어주는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "live_all_cryptocurrency_news_info.json 파일이 없으므로 새로 생성합니다.\n",
      "live_all_cryptocurrency_news_info.json에 데이터 저장 완료\n",
      "live_all_cryptocurrency_news_info.json에 데이터 저장 완료\n"
     ]
    }
   ],
   "source": [
    "def get_live_news_info(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # 페이지가 로딩되기를 기다림 (시간이 많이 걸릴 경우 조절 필요)\n",
    "\n",
    "        # 현재 페이지 소스코드 가져오기\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # BeautifulSoup을 사용하여 뉴스 제목과 올라온 시간 추출\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')    \n",
    "\n",
    "        news_info_list = []\n",
    "\n",
    "        # 뉴스 제목과 올라온 시간이 포함된 요소 선택\n",
    "        news_elements = soup.select('.largeTitle') \n",
    "\n",
    "        for element in news_elements:\n",
    "            # largeTitle 클래스 내에서 모든 .title과 .date를 찾음\n",
    "            titles = element.select('.title')\n",
    "            dates = element.select('.date')\n",
    "\n",
    "            # 뉴스 제목과 올라온 시간이 있는 경우에만 결과 리스트에 추가\n",
    "            for title, date in zip(titles, dates):\n",
    "                title_text = title.text.strip()\n",
    "                date_text = date.text.strip()\n",
    "                news_info_list.append({'title': title_text, 'time': date_text})\n",
    "\n",
    "        # 드라이버 종료\n",
    "        driver.quit()\n",
    "\n",
    "        return news_info_list\n",
    "\n",
    "    except Exception as e:\n",
    "        # 에러가 발생했을 때 현재 페이지 번호를 출력하고 예외를 다시 발생시킴\n",
    "        print(f\"에러가 발생했습니다. 페이지: {url}\")\n",
    "        raise e\n",
    "def save_to_json(data, filename='live_all_cryptocurrency_news_info.json'):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"{filename}에 데이터 저장 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {e}\")\n",
    "\n",
    "def long_polling(url, filename='live_all_cryptocurrency_news_info.json'):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"{filename} 파일이 없으므로 새로 생성합니다.\")\n",
    "        save_to_json({'news_info': []}, filename)\n",
    "\n",
    "    while True:\n",
    "        live_news_info = get_live_news_info(url)\n",
    "        if live_news_info:\n",
    "            try:\n",
    "                with open(filename, 'r', encoding='utf-8') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    data['news_info'].extend(live_news_info)\n",
    "                save_to_json(data, filename)\n",
    "            except Exception as e:\n",
    "                print(f\"파일 업데이트 중 오류 발생: {e}\")\n",
    "        time.sleep(10)  # 10초마다 다시 확인\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.investing.com/news/cryptocurrency-news/\"\n",
    "    long_polling(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
