{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- (원본)본문 접속 후 뉴스 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "def get_news_info(url, base_url=\"https://www.investing.com\"):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}  # 헤더 설정 추가\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        news_info_list = []\n",
    "\n",
    "        # 뉴스 제목과 올라온 시간이 포함된 요소 선택\n",
    "        news_elements = soup.select('.textDiv')\n",
    "\n",
    "        for element in news_elements:\n",
    "            title_element = element.select_one('.title')\n",
    "            date_element = element.select_one('.date')\n",
    "\n",
    "            if title_element and date_element:\n",
    "                title_text = title_element.text.strip()\n",
    "                date_text = date_element.text.strip()\n",
    "\n",
    "                # 상세 페이지 URL 구성\n",
    "                news_url = base_url + title_element['href']\n",
    "\n",
    "                # 상세 페이지로 이동하여 본문 내용을 크롤링\n",
    "                news_response = requests.get(news_url, headers=headers)\n",
    "                news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "                news_content = news_soup.find('div', class_='WYSIWYG articlePage').text.strip()\n",
    "\n",
    "                news_info_list.append({\n",
    "                    'title': title_text,\n",
    "                    'time': date_text,\n",
    "                    'content': news_content\n",
    "                })\n",
    "\n",
    "        return news_info_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"에러가 발생했습니다. 페이지: {url}\")\n",
    "        raise e\n",
    "\n",
    "# 여러 페이지의 뉴스 정보를 저장할 리스트\n",
    "all_news_info = []\n",
    "\n",
    "# 각 페이지에 대해 뉴스 정보를 가져와서 리스트에 추가\n",
    "for page_number in range(3, 4):\n",
    "    url = f\"https://www.investing.com/news/cryptocurrency-news/{page_number}\"\n",
    "    try:\n",
    "        news_info = get_news_info(url)\n",
    "        all_news_info.extend(news_info)\n",
    "        \n",
    "        # 너무 빠른 요청으로 인한 서버 부하를 방지하기 위해 대기\n",
    "        time.sleep(1)\n",
    "\n",
    "        print(f\"페이지 {page_number} 크롤링 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"에러가 발생하여 프로그램을 종료합니다. 페이지: {page_number}\")\n",
    "        break\n",
    "\n",
    "# 뉴스 정보를 JSON 파일로 저장\n",
    "result_data = {'news_info': all_news_info}\n",
    "with open('all_cryptocurrency_news_info.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(result_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\n총 {len(all_news_info)}개의 뉴스 정보 크롤링 완료 \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- (수정)본문 접속 후 뉴스 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_info(url, base_url=\"https://www.investing.com\"):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        news_info_list = []\n",
    "\n",
    "        # 뉴스 제목과 올라온 시간이 포함된 요소 선택\n",
    "        news_elements = soup.select('.largeTitle')\n",
    "\n",
    "        for element in news_elements:\n",
    "            # largeTitle 클래스 내에서 모든 .title과 .date를 찾음\n",
    "            titles = element.select('.title')\n",
    "            dates = element.select('.date')\n",
    "            \n",
    "            # 뉴스 제목과 올라온 시간이 있는 경우에만 결과 리스트에 추가\n",
    "            for title, date in zip(titles, dates):\n",
    "                title_text = title.text.strip()\n",
    "                date_text = date.text.strip()\n",
    "\n",
    "                # 상세 페이지 URL 구성\n",
    "                news_url = base_url + title['href']\n",
    "\n",
    "                # 상세 페이지로 이동하여 본문 내용과 업로드 시간을 크롤링\n",
    "                news_response = requests.get(news_url, headers=headers)\n",
    "                news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "                \n",
    "                # 본문 내용 가져오기\n",
    "                content_elements = news_soup.select('.WYSIWYG.articlePage > p, .WYSIWYG.articlePage > h2')\n",
    "                content_text = '\\n'.join([content.text.strip() for content in content_elements])\n",
    "\n",
    "                # 업로드 시간 가져오기\n",
    "                time_element = news_soup.select_one('.contentSectionDetails > span')\n",
    "                time_text = time_element.text.strip() if time_element else None\n",
    "\n",
    "                # 결과 리스트에 추가\n",
    "                news_info_list.append({\n",
    "                    'title': title_text,\n",
    "                    'time': time_text,\n",
    "                    'content': content_text\n",
    "                })\n",
    "\n",
    "        return news_info_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"에러가 발생했습니다. 페이지: {url}\")\n",
    "        raise e\n",
    "\n",
    "# 여러 페이지의 뉴스 정보를 저장할 리스트\n",
    "all_news_info = []\n",
    "\n",
    "# 각 페이지에 대해 뉴스 정보를 가져와서 리스트에 추가\n",
    "for page_number in range(4, 6):  # 페이지 범위 수정\n",
    "    url = f\"https://www.investing.com/news/cryptocurrency-news/{page_number}\"\n",
    "    try:\n",
    "        news_info = get_news_info(url)\n",
    "        all_news_info.extend(news_info)\n",
    "        \n",
    "        # 너무 빠른 요청으로 인한 서버 부하를 방지하기 위해 대기\n",
    "        time.sleep(1)\n",
    "\n",
    "        print(f\"페이지 {page_number} 크롤링 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"에러가 발생하여 프로그램을 종료합니다. 페이지: {page_number}\")\n",
    "        break\n",
    "\n",
    "# 총 몇 개의 데이터를 가져왔는지 출력\n",
    "print(f\"\\n총 {len(all_news_info)}개의 뉴스 정보 크롤링 완료\")\n",
    "\n",
    "# 뉴스 정보를 JSON 파일로 저장\n",
    "result_data = {'news_info': all_news_info}\n",
    "with open('all_crypto_news_info.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(result_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"뉴스 정보가 성공적으로 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
