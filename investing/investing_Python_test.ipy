import requests
from bs4 import BeautifulSoup
import json

<<<<<<< HEAD
=======
import requests
from bs4 import BeautifulSoup
import json

>>>>>>> 6b58b6645802f4db05fe5e8275ad2449b3ac7bd5
def get_news_info(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # 오류가 발생하면 예외를 발생시킴
        soup = BeautifulSoup(response.text, 'html.parser')

        news_info_list = []

        # 뉴스 제목과 올라온 시간이 포함된 요소 선택
        news_elements = soup.select('.largeTitle')

        for element in news_elements:
            # largeTitle 클래스 내에서 모든 .title과 .date를 찾음
            titles = element.select('.title')
            dates = element.select('.date')

            # 뉴스 제목과 올라온 시간이 있는 경우에만 결과 리스트에 추가
            for title, date in zip(titles, dates):
                title_text = title.text.strip()
                date_text = date.text.strip()
                news_info_list.append({'title': title_text, 'time': date_text})

        return news_info_list

    except Exception as e:
        # 에러가 발생했을 때 현재 페이지 번호를 출력하고 예외를 다시 발생시킴
        print(f"에러가 발생했습니다. 페이지: {url}")
        raise e

# 여러 페이지의 뉴스 정보를 저장할 리스트
all_news_info = []

# 1부터 2까지의 페이지에 대해 뉴스 정보를 가져와서 리스트에 추가
<<<<<<< HEAD
for page_number in range(3, 20):
=======
for page_number in range(1, 10):
>>>>>>> 6b58b6645802f4db05fe5e8275ad2449b3ac7bd5
    url = f"https://www.investing.com/news/cryptocurrency-news/{page_number}"
    try:
        news_info = get_news_info(url)
        all_news_info.extend(news_info)
    except Exception as e:
        # 에러가 발생했을 때 현재 페이지 번호를 출력하고 반복문을 종료
        print(f"에러가 발생하여 프로그램을 종료합니다. 페이지: {page_number}")
        break

# 총 몇개의 데이터를 가져왔는지 출력
print(f"\n총 {len(all_news_info)}개의 뉴스 정보 크롤링 완료 ")

# 뉴스 정보를 JSON 파일로 저장
result_data = {'news_info': all_news_info}
with open('2all_cryptocurrency_news_info.json', 'w', encoding='utf-8') as json_file:
    json.dump(result_data, json_file, ensure_ascii=False, indent=4)

print("뉴스 정보가 성공적으로 저장되었습니다.")