{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dateutil pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# 크롬드라이버 셋팅\n",
    "def set_chrome_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('headless')\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.investing.com/news/cryptocurrency-news/eth-etf-next-seasoned-vc-vance-spencer-shares-his-views-3290621'\n",
    "# driver 설정\n",
    "driver = set_chrome_driver(False)\n",
    "\n",
    "# URL 요청\n",
    "driver.get(url)\n",
    "\n",
    "# aritivlePage는 신문기사의 본문입니다\n",
    "article_page = driver.find_element(By.CLASS_NAME, 'articlePage')\n",
    "print(article_page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 59\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 날짜 정보를 찾는 셀렉터 사용\u001b[39;00m\n\u001b[0;32m     56\u001b[0m date_info \u001b[38;5;241m=\u001b[39m news_soup\u001b[38;5;241m.\u001b[39mselect_one(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#leftColumn > div:nth-child(6) > span:nth-child(1)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m temp_time \u001b[38;5;241m=\u001b[39m (\u001b[43mdate_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip())\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPublished \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m temp_time \u001b[38;5;241m=\u001b[39m temp_time\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m temp_article \u001b[38;5;241m=\u001b[39m (article_content\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "# import os\n",
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#     # This is the default and can be omitted\n",
    "#     api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "\n",
    "# def chat_func(prompt): \n",
    "#     chat_completion = client.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": prompt,\n",
    "#             }\n",
    "#         ],\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#     )\n",
    "#     return chat_completion.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "# Investing.com의 Cryptocurrency 뉴스 섹션 URL\n",
    "url = 'https://www.investing.com/news/cryptocurrency-news'\n",
    "\n",
    "# requests.get()에 사용할 헤더, 일부 사이트는 기본 Python user-agent를 차단할 수 있음\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# URL에서 HTML 콘텐츠를 가져옴\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# #leftColumn 내의 뉴스 링크를 찾음\n",
    "news_links = soup.select('#leftColumn div.textDiv a.title')\n",
    "\n",
    "# 뉴스 본문과 날짜를 저장할 딕셔너리\n",
    "news_time = []\n",
    "news_article = []\n",
    "\n",
    "for link in news_links:\n",
    "    # 상대 경로를 절대 경로로 변환\n",
    "    news_url = f\"https://www.investing.com{link['href']}\"\n",
    "    news_response = requests.get(news_url, headers=headers)\n",
    "    news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "    \n",
    "    # 본문 내용을 찾는 셀렉터 사용\n",
    "    article_content = news_soup.select_one('#leftColumn > div.WYSIWYG.articlePage')\n",
    "    \n",
    "    # 날짜 정보를 찾는 셀렉터 사용\n",
    "    date_info = news_soup.select_one(\"#leftColumn > div:nth-child(6) > span:nth-child(1)\")\n",
    "    \n",
    "    \n",
    "    temp_time = (date_info.text.strip()).replace(\"Published \", \"\")\n",
    "    temp_time = temp_time.replace(\" ET\", \"\")\n",
    "\n",
    "    temp_article = (article_content.text.strip()).replace(\",\", \" \")\n",
    "    temp_article = temp_article.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # # 프롬프트 (요약해줘 + 긍/부정 감정도 분석해줘)\n",
    "    # prompt = f'''\n",
    "    # Summarize the paragraph below and interpret whether it is a positive or negative sentiment.\n",
    "\n",
    "    # {article_content.text.strip()}\n",
    "    # '''\n",
    "\n",
    "    # print(chat_func(prompt))\n",
    "    news_time.append(temp_time)\n",
    "    news_article.append(temp_article)\n",
    "    \n",
    "    # 데모 목적으로 첫 번째 링크만 처리, 실제 사용 시 이 부분을 제거하세요\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Date': news_time,\n",
    "    'Content': news_article\n",
    "})\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# df['Date'] = df['Date'].dt.tz_localize('UTC').tz_convert('Asia/Seoul')\n",
    "df\n",
    "csv_path = 'D:\\csv\\hist_news\\hist_news_data.csv'  # 저장할 CSV 파일의 경로와 파일명 지정\n",
    "df.to_csv(csv_path, index=False, quotechar='\"', quoting=csv.QUOTE_ALL, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "62 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n",
      "63 번째 페이지 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# 뉴스 본문과 날짜를 저장할 리스트 초기화\n",
    "news_header = []\n",
    "news_time = []\n",
    "news_article = []\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# 62페이지부터 3819페이지까지 반복\n",
    "for page in range(62, 64):\n",
    "    # 페이지 번호를 포함한 URL\n",
    "    url = f'https://www.investing.com/news/cryptocurrency-news/{page}'\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 현재 페이지 내의 뉴스 링크를 찾음\n",
    "    news_links = soup.select('#leftColumn div.textDiv a.title')\n",
    "    \n",
    "    for link in news_links:\n",
    "        # 상대 경로를 절대 경로로 변환\n",
    "        news_url = f\"https://www.investing.com{link['href']}\"\n",
    "        news_response = requests.get(news_url, headers=headers)\n",
    "        news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "        \n",
    "        title_element = news_soup.select_one(\"#leftColumn > h1\")\n",
    "        if title_element:\n",
    "            news_header.append(title_element.text.strip())\n",
    "        \n",
    "            # 본문 내용을 찾는 셀렉터 사용\n",
    "            article_content = news_soup.select_one('#leftColumn > div.WYSIWYG.articlePage')\n",
    "\n",
    "            # article_content가 None이 아닐 때만 처리를 계속 진행\n",
    "            if article_content:\n",
    "                exclude_element = article_content.select_one('#imgCarousel > span')\n",
    "                if exclude_element:\n",
    "                    exclude_element.decompose()\n",
    "                \n",
    "                exclude_element_relatedInstruments = article_content.select_one('div.relatedInstrumentsWrapper > div')\n",
    "                if exclude_element_relatedInstruments:\n",
    "                    exclude_element_relatedInstruments.decompose()\n",
    "            \n",
    "            # 날짜 정보를 찾는 셀렉터 사용\n",
    "            date_info = news_soup.select_one(\"#leftColumn > div:nth-child(6) > span:nth-child(1)\")\n",
    "            \n",
    "            if date_info and article_content:\n",
    "                temp_time = (date_info.text.strip()).replace(\"Published \", \"\").replace(\" ET\", \"\")\n",
    "                temp_article = (article_content.text.strip()).replace(\",\", \" \").replace(\"\\n\", \"\")\n",
    "                \n",
    "                news_time.append(temp_time)\n",
    "                news_article.append(temp_article)\n",
    "        print(page,'번째 페이지 저장 완료')\n",
    "        \n",
    "\n",
    "# 데이터 프레임 생성\n",
    "df = pd.DataFrame({\n",
    "    'Date': news_time,\n",
    "    'Header' : news_header,\n",
    "    'Content': news_article\n",
    "})\n",
    "\n",
    "# 날짜 형식 변환\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df = df.sort_values(by='Date', ascending=True)  # 날짜 기준으로 오름차순 정렬\n",
    "df = df.drop_duplicates(subset='Header', keep='first')  # 제목 기준으로 중복 제거, 가장 오래된 것을 유지\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_path = 'D:\\csv\\hist_news\\hist_news_data.csv'\n",
    "df.to_csv(csv_path, index=False, quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
