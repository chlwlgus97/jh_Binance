{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dateutil pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# 크롬드라이버 셋팅\n",
    "def set_chrome_driver(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('headless')\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.investing.com/news/cryptocurrency-news/eth-etf-next-seasoned-vc-vance-spencer-shares-his-views-3290621'\n",
    "# driver 설정\n",
    "driver = set_chrome_driver(False)\n",
    "\n",
    "# URL 요청\n",
    "driver.get(url)\n",
    "\n",
    "# aritivlePage는 신문기사의 본문입니다\n",
    "article_page = driver.find_element(By.CLASS_NAME, 'articlePage')\n",
    "print(article_page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 59\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 날짜 정보를 찾는 셀렉터 사용\u001b[39;00m\n\u001b[0;32m     56\u001b[0m date_info \u001b[38;5;241m=\u001b[39m news_soup\u001b[38;5;241m.\u001b[39mselect_one(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#leftColumn > div:nth-child(6) > span:nth-child(1)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m temp_time \u001b[38;5;241m=\u001b[39m (\u001b[43mdate_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip())\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPublished \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m temp_time \u001b[38;5;241m=\u001b[39m temp_time\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m temp_article \u001b[38;5;241m=\u001b[39m (article_content\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "# import os\n",
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#     # This is the default and can be omitted\n",
    "#     api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "\n",
    "# def chat_func(prompt): \n",
    "#     chat_completion = client.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": prompt,\n",
    "#             }\n",
    "#         ],\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#     )\n",
    "#     return chat_completion.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "# Investing.com의 Cryptocurrency 뉴스 섹션 URL\n",
    "url = 'https://www.investing.com/news/cryptocurrency-news'\n",
    "\n",
    "# requests.get()에 사용할 헤더, 일부 사이트는 기본 Python user-agent를 차단할 수 있음\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# URL에서 HTML 콘텐츠를 가져옴\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# #leftColumn 내의 뉴스 링크를 찾음\n",
    "news_links = soup.select('#leftColumn div.textDiv a.title')\n",
    "\n",
    "# 뉴스 본문과 날짜를 저장할 딕셔너리\n",
    "news_time = []\n",
    "news_article = []\n",
    "\n",
    "for link in news_links:\n",
    "    # 상대 경로를 절대 경로로 변환\n",
    "    news_url = f\"https://www.investing.com{link['href']}\"\n",
    "    news_response = requests.get(news_url, headers=headers)\n",
    "    news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "    \n",
    "    # 본문 내용을 찾는 셀렉터 사용\n",
    "    article_content = news_soup.select_one('#leftColumn > div.WYSIWYG.articlePage')\n",
    "    \n",
    "    # 날짜 정보를 찾는 셀렉터 사용\n",
    "    date_info = news_soup.select_one(\"#leftColumn > div:nth-child(6) > span:nth-child(1)\")\n",
    "    \n",
    "    \n",
    "    temp_time = (date_info.text.strip()).replace(\"Published \", \"\")\n",
    "    temp_time = temp_time.replace(\" ET\", \"\")\n",
    "\n",
    "    temp_article = (article_content.text.strip()).replace(\",\", \" \")\n",
    "    temp_article = temp_article.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # # 프롬프트 (요약해줘 + 긍/부정 감정도 분석해줘)\n",
    "    # prompt = f'''\n",
    "    # Summarize the paragraph below and interpret whether it is a positive or negative sentiment.\n",
    "\n",
    "    # {article_content.text.strip()}\n",
    "    # '''\n",
    "\n",
    "    # print(chat_func(prompt))\n",
    "    news_time.append(temp_time)\n",
    "    news_article.append(temp_article)\n",
    "    \n",
    "    # 데모 목적으로 첫 번째 링크만 처리, 실제 사용 시 이 부분을 제거하세요\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Date': news_time,\n",
    "    'Content': news_article\n",
    "})\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# df['Date'] = df['Date'].dt.tz_localize('UTC').tz_convert('Asia/Seoul')\n",
    "df\n",
    "csv_path = 'D:\\csv\\hist_news\\hist_news_data.csv'  # 저장할 CSV 파일의 경로와 파일명 지정\n",
    "df.to_csv(csv_path, index=False, quotechar='\"', quoting=csv.QUOTE_ALL, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "64 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n",
      "65 번째 페이지 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# 뉴스 본문과 날짜를 저장할 리스트 초기화\n",
    "news_header = []\n",
    "news_time = []\n",
    "news_article = []\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# 62페이지부터 3819페이지까지 반복\n",
    "for page in range(64, 66):\n",
    "    # 페이지 번호를 포함한 URL\n",
    "    url = f'https://www.investing.com/news/cryptocurrency-news/{page}'\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 현재 페이지 내의 뉴스 링크를 찾음\n",
    "    news_links = soup.select('#leftColumn div.textDiv a.title')\n",
    "    \n",
    "    for link in news_links:\n",
    "        # 상대 경로를 절대 경로로 변환\n",
    "        news_url = f\"https://www.investing.com{link['href']}\"\n",
    "        news_response = requests.get(news_url, headers=headers)\n",
    "        news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "        \n",
    "        title_element = news_soup.select_one(\"#leftColumn > h1\")\n",
    "        if title_element:\n",
    "            news_header.append(title_element.text.strip())\n",
    "        \n",
    "            # 본문 내용을 찾는 셀렉터 사용\n",
    "            article_content = news_soup.select_one('#leftColumn > div.WYSIWYG.articlePage')\n",
    "\n",
    "            # article_content가 None이 아닐 때만 처리를 계속 진행\n",
    "            if article_content:\n",
    "                exclude_element = article_content.select_one('#imgCarousel > span')\n",
    "                if exclude_element:\n",
    "                    exclude_element.decompose()\n",
    "                \n",
    "                exclude_element_relatedInstruments = article_content.select_one('div.relatedInstrumentsWrapper > div')\n",
    "                if exclude_element_relatedInstruments:\n",
    "                    exclude_element_relatedInstruments.decompose()\n",
    "            \n",
    "            # 날짜 정보를 찾는 셀렉터 사용\n",
    "            date_info = news_soup.select_one(\"#leftColumn > div:nth-child(6) > span:nth-child(1)\")\n",
    "            \n",
    "            if date_info and article_content:\n",
    "                temp_time = (date_info.text.strip()).replace(\"Published \", \"\").replace(\" ET\", \"\")\n",
    "                temp_article = (article_content.text.strip()).replace(\",\", \" \").replace(\"\\n\", \"\")\n",
    "                \n",
    "                news_time.append(temp_time)\n",
    "                news_article.append(temp_article)\n",
    "        print(page,'번째 페이지 저장 완료')\n",
    "        \n",
    "\n",
    "# 데이터 프레임 생성\n",
    "df = pd.DataFrame({\n",
    "    'Date': news_time,\n",
    "    'Header' : news_header,\n",
    "    'Content': news_article\n",
    "})\n",
    "\n",
    "# 날짜 형식 변환\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df = df.sort_values(by='Date', ascending=True)  # 날짜 기준으로 오름차순 정렬\n",
    "df = df.drop_duplicates(subset='Header', keep='first')  # 제목 기준으로 중복 제거, 가장 오래된 것을 유지\n",
    "\n",
    "# CSV 파일로 저장\n",
    "csv_path = 'C:\\\\education\\\\ml_dev\\\\coin\\\\jh_-\\\\investing\\\\investing_news.csv'\n",
    "df.to_csv(csv_path, index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- rss feed로 crypto 뉴스 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 30.7/81.3 kB 640.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 81.3/81.3 kB 911.6 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=def943e949f47e71abe7f7584c15eb6efaf0a23cba29d67c471102b60da1cac1\n",
      "  Stored in directory: c:\\users\\pc\\appdata\\local\\pip\\cache\\wheels\\f0\\69\\93\\a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# RSS 피드 URL\n",
    "rss_url = \"https://www.investing.com/rss/news_301.rss\"\n",
    "\n",
    "def get_new_crypto_news(rss_url):\n",
    "    # 이전에 가져온 뉴스의 제목 저장\n",
    "    previous_news_titles = set()\n",
    "    \n",
    "    while True:\n",
    "        # RSS 피드 가져오기\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        \n",
    "        # 새로운 뉴스를 저장할 데이터프레임 생성\n",
    "        new_news_df = pd.DataFrame(columns=['title', 'upload_time', 'summary'])\n",
    "        \n",
    "        # 각 항목에 대해 반복하며 새로운 뉴스 확인\n",
    "        for entry in feed.entries:\n",
    "            title = entry.title\n",
    "            upload_time = entry.published\n",
    "            # 뉴스 본문 링크\n",
    "            link = entry.link\n",
    "            \n",
    "            # 뉴스 본문 가져오기\n",
    "            # 본문의 내용은 entry.summary 혹은 entry.description 등을 사용하여 가져올 수 있습니다.\n",
    "            # 본문의 구체적인 형식은 RSS 피드의 형식에 따라 다를 수 있습니다.\n",
    "            # 이 예시에서는 entry.summary를 사용합니다.\n",
    "            if hasattr(entry, 'summary'):\n",
    "                summary = entry.summary\n",
    "            else:\n",
    "                summary = None\n",
    "            \n",
    "            # 새로운 뉴스인지 확인\n",
    "            if title not in previous_news_titles:\n",
    "                # 새로운 뉴스 데이터프레임에 추가\n",
    "                new_news_df = pd.concat([new_news_df, pd.DataFrame({'title': [title], 'upload_time': [upload_time], 'summary': [summary]})], ignore_index=True)\n",
    "                \n",
    "                # 새로운 뉴스 제목을 저장하여 중복 확인 용도로 사용\n",
    "                previous_news_titles.add(title)\n",
    "        \n",
    "        # CSV 파일로 저장 (append 모드로)\n",
    "        with open(\"crypto_news.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "            new_news_df.to_csv(file, header=False, index=False)\n",
    "        \n",
    "        # 1분마다 RSS 피드 다시 가져오기\n",
    "        time.sleep(60)\n",
    "\n",
    "# 실시간으로 새로운 뉴스 확인하고 데이터프레임에 저장 및 CSV 파일에 저장\n",
    "get_new_crypto_news(rss_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
